
@unpublished{anderliniDensityEstimationTrees2015,
  title = {Density Estimation Trees in High Energy Physics},
  author = {Anderlini, Lucio},
  date = {2015-02-03},
  eprint = {1502.00932},
  eprinttype = {arxiv},
  primaryclass = {hep-ex, physics:physics, stat},
  url = {http://arxiv.org/abs/1502.00932},
  urldate = {2022-08-18},
  abstract = {Density Estimation Trees can play an important role in exploratory data analysis for multidimensional, multi-modal data models of large samples. I briefly discuss the algorithm, a self-optimization technique based on kernel density estimation, and some applications in High Energy Physics.},
  archiveprefix = {arXiv},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/5IIVD8MZ/Anderlini -- Density Estimation Trees in High Energy Physics.pdf}
}

@article{anderliniDensityEstimationTrees2016,
  title = {Density Estimation Trees as fast non-parametric modelling tools},
  author = {Anderlini, Lucio},
  date = {2016-10},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {762},
  eprint = {1607.06635},
  eprinttype = {arxiv},
  pages = {012042},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/762/1/012042},
  url = {http://arxiv.org/abs/1607.06635},
  urldate = {2022-08-18},
  abstract = {Density Estimation Trees (DETs) are decision trees trained on a multivariate dataset to estimate its probability density function. While not competitive with kernel techniques in terms of accuracy, they are incredibly fast, embarrassingly parallel and relatively small when stored to disk. These properties make DETs appealing in the resource-expensive horizon of the LHC data analysis. Possible applications may include selection optimization, fast simulation and fast detector calibration. In this contribution I describe the algorithm, made available to the HEP community in a RooFit implementation. A set of applications under discussion within the LHCb Collaboration are also briefly illustrated.},
  archiveprefix = {arXiv},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/KMXMKVME/Anderlini -- Density Estimation Trees as fast non-parametric mo.pdf}
}

@article{cousinsCaDETInterpretableParametric2019,
  title = {CaDET: interpretable parametric conditional density estimation with decision trees and forests},
  shorttitle = {CaDET},
  author = {Cousins, Cyrus and Riondato, Matteo},
  date = {2019-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {108},
  number = {8-9},
  pages = {1613--1634},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-019-05820-3},
  url = {http://link.springer.com/10.1007/s10994-019-05820-3},
  urldate = {2022-08-18},
  abstract = {We introduce CaDET, an algorithm for parametric Conditional Density Estimation (CDE) based on decision trees and random forests. CaDET uses the empirical cross entropy impurity criterion for tree growth, which incentivizes splits that improve predictive accuracy more than the regression criteria or estimated mean-integrated-square-error used in previous works. CaDET also admits more efficient training and query procedures than existing tree-based CDE approaches, and stores only a bounded amount of information at each tree leaf, by using sufficient statistics for all computations. Previous tree-based CDE techniques produce complicated uninterpretable distribution objects, whereas CaDET may be instantiated with easily interpretable distribution families, making every part of the model easy to understand. Our experimental evaluation on real datasets shows that CaDET usually learns more accurate, smaller, and more interpretable models, and is less prone to overfitting than existing tree-based CDE approaches.},
  langid = {english},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/GSPJR2ES/Cousins and Riondato -- CaDET interpretable parametric conditional densit.pdf}
}

@unpublished{gohCascadedHighDimensional2016,
  title = {Cascaded High Dimensional Histograms: A Generative Approach to Density Estimation},
  shorttitle = {Cascaded High Dimensional Histograms},
  author = {Goh, Siong Thye and Rudin, Cynthia},
  date = {2016-02-12},
  eprint = {1510.06779},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1510.06779},
  urldate = {2022-08-18},
  abstract = {We present tree- and list- structured density estimation methods for high dimensional binary/categorical data. Our density estimation models are high dimensional analogies to variable bin width histograms. In each leaf of the tree (or list), the density is constant, similar to the flat density within the bin of a histogram. Histograms, however, cannot easily be visualized in higher dimensions, whereas our models can. The accuracy of histograms fades as dimensions increase, whereas our models have priors that help with generalization. Our models are sparse, unlike high-dimensional histograms. We present three generative models, where the first one allows the user to specify the number of desired leaves in the tree within a Bayesian prior. The second model allows the user to specify the desired number of branches within the prior. The third model returns lists (rather than trees) and allows the user to specify the desired number of rules and the length of rules within the prior. Our results indicate that the new approaches yield a better balance between sparsity and accuracy of density estimates than other methods for this task.},
  archiveprefix = {arXiv},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/9Q983W4L/Goh and Rudin -- Cascaded High Dimensional Histograms A Generative.pdf}
}

@inproceedings{ramDensityEstimationTrees2011,
  title = {Density estimation trees},
  booktitle = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
  author = {Ram, Parikshit and Gray, Alexander G.},
  date = {2011},
  pages = {627},
  publisher = {ACM Press},
  location = {San Diego, California, USA},
  doi = {10.1145/2020408.2020507},
  url = {http://dl.acm.org/citation.cfm?doid=2020408.2020507},
  urldate = {2022-08-18},
  abstract = {In this paper we develop density estimation trees (DETs), the natural analog of classification trees and regression trees, for the task of density estimation. We consider the estimation of a joint probability density function of a d-dimensional random vector X and define a piecewise constant estimator structured as a decision tree. The integrated squared error is minimized to learn the tree. We show that the method is nonparametric: under standard conditions of nonparametric density estimation, DETs are shown to be asymptotically consistent. In addition, being decision trees, DETs perform automatic feature selection. They empirically exhibit the interpretability, adaptability and feature selection properties of supervised decision trees while incurring slight loss in accuracy over other nonparametric density estimators. Hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimensions. We believe that density estimation trees provide a new tool for exploratory data analysis with unique capabilities.},
  eventtitle = {the 17th ACM SIGKDD international conference},
  isbn = {978-1-4503-0813-7},
  langid = {english},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/S47D7XDL/Ram and Gray -- Density estimation trees.pdf}
}

@book{silvermanDensityEstimationStatistics1986,
  title = {Density Estimation for Statistics and Data Analysis},
  author = {Silverman, B. W.},
  date = {1986},
  publisher = {Routledge},
  doi = {10.1201/9781315140919},
  url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315140919/density-estimation-statistics-data-analysis-silverman},
  urldate = {2022-08-20},
  abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician.The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text.Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.},
  isbn = {978-1-315-14091-9},
  langid = {english},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/73V53AT4/Density Estimation for Statistics and Data Analysi.pdf}
}

@unpublished{wellsSimpleEfficientDensity2017,
  ids = {wellsNewSimpleEfficient2019},
  title = {A simple efficient density estimator that enables fast systematic search},
  author = {Wells, Jonathan R. and Ting, Kai Ming},
  date = {2017-09-12},
  eprint = {1707.00783},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.00783},
  urldate = {2022-08-18},
  abstract = {This paper introduces a simple and efficient density estimator that enables fast systematic search. To show its advantage over commonly used kernel density estimator, we apply it to outlying aspects mining. Outlying aspects mining discovers feature subsets (or subspaces) that describe how a query stand out from a given dataset. The task demands a systematic search of subspaces. We identify that existing outlying aspects miners are restricted to datasets with small data size and dimensions because they employ kernel density estimator, which is computationally expensive, for subspace assessments. We show that a recent outlying aspects miner can run orders of magnitude faster by simply replacing its density estimator with the proposed density estimator, enabling it to deal with large datasets with thousands of dimensions that would otherwise be impossible.},
  archiveprefix = {arXiv},
  file = {/home/balodja/.zotero/zotero/gcofuz5j.default/zotero/storage/UQ6BGUXD/Wells and Ting -- A simple efficient density estimator that enables .pdf}
}


